"""
Semantic Memory Filter for Open WebUI

This filter implements semantic memory - the storage and retrieval of factual
knowledge, concepts, and learned information across conversations. Unlike episodic
memory (which stores specific exchanges), semantic memory extracts and preserves
facts, preferences, and conceptual understanding.

Purpose:
    Enable AI systems to:
    - Learn facts about users persistently
    - Build knowledge graphs of concepts
    - Maintain factual consistency across sessions
    - Accumulate domain expertise over time
    
    Example semantic memories:
    - "User prefers Python over JavaScript"
    - "User works in machine learning research"
    - "User's timezone is America/Chicago"
    - "User is interested in AI consciousness"

Architecture:
    Stores factual statements as semantic embeddings in Qdrant. Uses similarity
    search to retrieve relevant knowledge when topics are discussed.

Memory Format:
    {
        "memory_id": "se_m4n3o2p1",
        "collection": "semantic",
        "timestamp": "2025-11-04T20:30:00Z",
        "content": {
            "fact": "User studies psychiatry",
            "context": "Mentioned during discussion about mental health",
            "confidence": 0.95
        }
    }

Key Differences from Episodic:
    - Episodic: "On Tuesday, user said they like coffee"
    - Semantic: "User likes coffee"
    
    - Episodic: Specific conversations and exchanges
    - Semantic: Extracted facts and knowledge
    
    - Episodic: Temporal and contextual
    - Semantic: Timeless and conceptual

Usage:
    This filter extracts factual information from conversations and stores it
    for long-term knowledge accumulation. Retrieved facts inform future responses.

Technical Details:
    - Embedding Model: mixedbread-ai/mxbai-embed-large-v1 (1024 dims)
    - Similarity: Cosine distance with higher threshold (0.6 vs 0.5)
    - Extraction: Currently manual, future: automatic fact extraction
    - Storage: Qdrant vector database

Future Enhancements:
    - Automatic fact extraction from conversations
    - Confidence scoring for facts
    - Contradiction detection and resolution
    - Knowledge graph construction

Author: dotjax
License: GPL-3.0
Repository: https://github.com/dotjax/open-webui-memory-layers
"""

from __future__ import annotations

import time
import traceback

from typing import Callable, Dict, MutableSequence, Optional, TypeVar

from pydantic import BaseModel, Field
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer

MessageList = MutableSequence[Dict[str, str]]
T = TypeVar("T")

_MODEL_CACHE: Dict[str, SentenceTransformer] = {}
_QDRANT_CACHE: Dict[str, QdrantClient] = {}


def _generate_memory_id(collection: str, existing_id: Optional[str] = None) -> str:
    """Generate a unique memory ID with collection prefix."""
    import uuid
    if existing_id:
        return f"{collection[:2]}_{existing_id[:8]}"
    return f"{collection[:2]}_{uuid.uuid4().hex[:8]}"


def _format_memories_json(memories: list[dict[str, any]]) -> str:
    """Format multiple memories as JSON array string."""
    import json
    from datetime import datetime, timezone
    if not memories:
        return "[]"
    
    memory_objects = []
    for memory in memories:
        memory_id = _generate_memory_id(
            memory["collection"], 
            memory.get("existing_id")
        )
        
        memory_obj = {
            "memory_id": memory_id,
            "collection": memory["collection"],
            "timestamp": memory.get("timestamp", datetime.now(timezone.utc).isoformat()),
            "content": memory["content"]
        }
        memory_objects.append(memory_obj)
    
    return json.dumps(memory_objects, indent=2)


def append_system_context(messages: MessageList, context: str) -> None:
    if not context:
        return
    if not messages:
        messages.insert(0, {"role": "system", "content": context})
        return
    first_message = messages[0]
    if first_message.get("role") == "system":
        first_message["content"] = f"{first_message.get('content', '')}{context}"
    else:
        messages.insert(0, {"role": "system", "content": context})


def run_qdrant_operation(
    operation: Callable[[], T],
    log: Callable[[str, str], None],
    *,
    description: str,
    retries: int = 1,
    backoff_seconds: float = 0.5,
) -> T:
    last_error: Optional[Exception] = None
    for attempt in range(retries + 1):
        try:
            return operation()
        except Exception as exc:                
            last_error = exc
            log(f"{description} failed (attempt {attempt + 1}/{retries + 1}): {exc}", "ERROR")
            if attempt == retries:
                raise
            time.sleep(backoff_seconds)
    raise RuntimeError(f"{description} failed after {retries + 1} attempts") from last_error


class Filter:
    class Valves(BaseModel):
        qdrant_host: str = Field(
            default="localhost",
            description="Qdrant server host"
        )
        qdrant_port: int = Field(
            default=6333,
            description="Qdrant server port"
        )
        collection_name: str = Field(
            default="semantic",
            description="Qdrant collection name for semantic memories"
        )
        embedding_model: str = Field(
            default="/home/ubuntu/dev/models/embedding/mxbai-embed-large-v1",
            description="Sentence transformer model for embeddings"
        )
        embedding_device: str = Field(
            default="cpu",
            description="Device for embedding model (cpu/cuda)"
        )
        top_k_summaries: int = Field(
            default=3,
            description="Number of semantic summaries to retrieve"
        )
        enabled: bool = Field(
            default=True,
            description="Enable/disable semantic knowledge retrieval"
        )
        debug_logging: bool = Field(
            default=True,
            description="Enable detailed debug logging"
        )

    def __init__(self) -> None:
        self.valves = self.Valves()
        self._qdrant: Optional[QdrantClient] = None
        self._embedding_model: Optional[SentenceTransformer] = None

    def _log(self, message: str, level: str = "INFO"):
        if level == "DEBUG" and not self.valves.debug_logging:
            return
        print(f"[Semantic Knowledge] {level}: {message}")

    @property
    def qdrant(self) -> QdrantClient:
        cache_key = f"{self.valves.qdrant_host}:{self.valves.qdrant_port}"
        if cache_key not in _QDRANT_CACHE:
            try:
                self._log(
                    f"Connecting to Qdrant at {self.valves.qdrant_host}:{self.valves.qdrant_port} (FIRST CONNECTION - caching in memory)",
                    "INFO",
                )
                _QDRANT_CACHE[cache_key] = QdrantClient(
                    host=self.valves.qdrant_host,
                    port=self.valves.qdrant_port,
                    timeout=5.0
                )
                run_qdrant_operation(
                    _QDRANT_CACHE[cache_key].get_collections,
                    self._log,
                    description="Qdrant get_collections",
                )
                self._log("Qdrant client cached in memory permanently", "DEBUG")
            except Exception as exc:
                self._log(f"Failed to connect to Qdrant: {exc}", "ERROR")
                raise
        else:
            self._log("Using cached Qdrant client", "DEBUG")
        return _QDRANT_CACHE[cache_key]

    @property
    def embedding_model(self) -> SentenceTransformer:
        cache_key = f"{self.valves.embedding_model}_{self.valves.embedding_device}"
        if cache_key not in _MODEL_CACHE:
            try:
                self._log(
                    f"Loading embedding model: {self.valves.embedding_model} (FIRST LOAD - caching in memory)",
                    "INFO",
                )
                _MODEL_CACHE[cache_key] = SentenceTransformer(
                    self.valves.embedding_model,
                    device=self.valves.embedding_device,
                )
                self._log("Embedding model cached in memory permanently", "DEBUG")
            except Exception as exc:
                self._log(f"Failed to load embedding model: {exc}", "ERROR")
                raise
        else:
            self._log("Using cached embedding model", "DEBUG")
        return _MODEL_CACHE[cache_key]

    def inlet(self, body: dict, __user__: dict) -> dict:
        if not self.valves.enabled:
            self._log("Semantic knowledge retrieval disabled via Valves", "DEBUG")
            return body
        try:
            messages = body.get("messages", [])
            if not messages:
                self._log("No messages in body", "DEBUG")
                return body
            last_message = messages[-1]
            if last_message.get("role") != "user":
                self._log("Last message is not from user", "DEBUG")
                return body
            user_message = last_message.get("content", "")
            if not user_message or not user_message.strip():
                self._log("User message is empty", "DEBUG")
                return body
            assistant_message = ""
            for msg in reversed(messages[:-1]):
                if msg.get("role") == "assistant":
                    assistant_message = msg.get("content", "")
                    break
            self._log(f"Processing user message: {user_message[:100]}...", "DEBUG")
            combined_text = f"{user_message}\n{assistant_message}"
            self._log("Generating embedding for combined messages", "DEBUG")
            embedding = self.embedding_model.encode(combined_text)
            self._log("Searching semantic collection", "DEBUG")
            results = run_qdrant_operation(
                lambda: self.qdrant.search(
                    collection_name=self.valves.collection_name,
                    query_vector=embedding,
                    limit=self.valves.top_k_summaries,
                    with_payload=True
                ),
                self._log,
                description="Qdrant semantic search",
            )
            if not results:
                self._log("No semantic summaries found", "DEBUG")
                return body
            
            # Transform semantic summaries to JSON format
            json_memories = []
            for result in results:
                summary_text = result.payload.get("semantic_summary", "")
                if summary_text:
                    # Create structured content
                    memory_content = {
                        "semantic_summary": summary_text,
                        "relevance_score": result.score,
                        "knowledge_type": "semantic",
                        "source": "knowledge_base"
                    }
                    
                    json_memories.append({
                        "collection": "semantic",
                        "content": memory_content,
                        "timestamp": None,  # Semantic knowledge doesn't have timestamps
                        "existing_id": str(result.id)
                    })
            
            if not json_memories:
                self._log("No valid semantic summaries found", "DEBUG")
                return body
            
            context = _format_memories_json(json_memories)
            append_system_context(body["messages"], context)
            self._log(f"Injected {len(json_memories)} semantic summaries into context", "DEBUG")
        except Exception as exc:
            self._log(f"Error in inlet(): {exc}", "ERROR")
            if self.valves.debug_logging:
                self._log(traceback.format_exc(), "ERROR")
        return body